一、介绍缓存
	· 缓存的作用就是对数据的快速访问，将部分数据放置到快速且空间有限的内存当中；
	  所以缓存的核心就是在于将尽量高频的数据放到内存中，也就是缓存淘汰策略；
	  缓存就是Map+淘汰策略；Map提供快速访问，淘汰策略是缓存算法的灵魂，决定了命中率的高低；

	· 缓存在KV引擎中的作用在于将打开的sstable文件对象和相关元数据，避免每次访问数据都要重新打开sstable；
	  还有一点在于将sstable中dataBlock的内存，可以加快对Block的访问；

	· LRU算法：
		· LRU Least Recently Used 最近最少使用算法；
		  LRU的思想是如果一个数据在最近一段时间没有被使用，那么在将来的一段时间被访问的可能性也会很小；
		  所以，当指定的缓存空间已经存满数据时，就应该把最久没有使用的数据淘汰掉；
		  也就是淘汰数据时，只看数据在缓存中上一次被使用的时间这一单一维度；

		· 例子：假设缓存中最大容量为3，访问顺序为abcaaaabcd；
			a -- ab -- abc -- bca -- bca -- bca -- bca -- cab -- abc -- bcd；

		· 通过上面的例子，可以发现LRU存在的缺点，明明a才是被频繁访问的数据，但是却被很容易的替换出去了；
		  自然而然，如果要改进这算法，希望是能够考虑到每个元素的访问频率的信息，访问频率最低的元素才是最应该被淘汰的那个；
		  这就是LFU算法
	
		· 伪代码：
			根据LRU的描述，可以使用链表来表示数据最近访问时间的顺序
				type LRU struct{
					data map[string]*list.node
					cap int
					list *list.List
				}
			cap是缓存中可以存放的数据个数，也就是缓存的容量上限；
			data就是Map，用于快速查找数据；
			List用于记录数据先后访问顺序，每次范围跟都把本次访问的节点移动到链表中的头部；
			这样整个链表就可以按照近期访问记录的顺序来排序了；
				func (lru *LRU) add(key,value string){
					if data中存在这个key{
						替换掉Map中的value值
						将list中对应的节点移到list的最前面
					}else{
						if 已经达到缓存容量上限{
							找到list的尾部节点的key，从map中删除
							移除list尾部的node
						}
						创建对应的新的node
						将新的node放到map的头部
						插入到map中
					}
				}
				func (lru *LRU) get(k string)string{
					if map中存在这个key{
						返回查询的value
						将list中对应的节点移动到list的头部
					}else{
						返回 nill
					}
				}

		· LFU算法：
			· LFU算法相比于LRU算法，多了能够记录访问次数的信息，并且按照访问次数从高到低排序；
			  访问次数可以通过在listnode中添加一个访问频率frequency字段，这个frequency可以使用int来存储；
			  同时排序规则修改为按照访问频率插入到对应的位置，如果频率相同，再按照LRU的规则比较谁是最新访问的；

			· 例子：假设缓存中最大容量为3，访问顺序为abcaaaabcd；	
				a -- ab -- abc -- bca -- bca -- bca -- bca -- cba -- bca -- dca；
			
			· 优点：对于热点命中率更高
			  缺点：难以应对突发的稀疏流量、可能存在旧数据长期不被淘汰，会影响某些场景下的命中率(例如外卖早上点包子的多，中午点饭的多)，一些过时的数据很难被替换出去；
			        同时需要消耗额外的空间来存储记录数据，以及需要消耗额外的性能来解决并发的问题

		· TinyLFU算法：
			· TinyLFU解决了上面提到的问题：
				1. LFU需要额外的消耗来存储统计信息
				2. LFU存在就数据长期不被淘汰的问题
			  不过TinyLFU仍然欸有解决难以应对突发的稀疏流量的问题，所以随后又引入了W-TinyLFU；

			· W-TinyLFU的窗口设计；
				· 主缓存(main chahe)使用SLRU淘汰策略和TinyLFU接纳策略，而窗口缓存(window cache)采用LRU淘汰策略而没有任何接纳策略；
				
				· main cache根据SLRU策略静态划分为A1和A2两个区域，80%的空间分配的热门项目A2，并从20%的非热门项目(A1)中挑选Victim；
				  所有请求的key都会被允许进入窗口缓存，而窗口缓存的victim则有机会被允许进入main cache；

二、TinyLFU
	· Count-Min Sketch算法：
		· 上面提到了LFU需要统计每条数据的访问频率，这就需要一个int类型或者其他类型来存储数据；
		  但是在实际过程中一个缓存被访问15次可能就算比较高了，那么只需要使用4bits既可以保存这数据；

		· 根据BlommFilter的思想，可以用bitmap来表示一条数据是否存在，那么也可以利用这个思想来对某个对象计数；
		  首先给要计数的值计算一个hashval，然后在bitmap的对应位置累计加一就可以了；
		  也就是说，在blommfilter中1个bit表示一个信息，但是在count-min sketch中使用4个bit来表示信息；

		· 代码实现：
			type cmRow []byte

			// 因为使用4bits来表示一个counter
			// 如果需要64个counter，就需要申请32byte的空间
			func NewCmRow(counterSize int64) cmRow {
				return make(cmRow, counterSize/2)
			}
			func (r cmRow) incrRow(offset int) {
				// 计算应该放在哪个byte中
				byteOffset := offset / 2
				// 计算对应的是byte的前4位还是后四位
				counterOffset := (offset & 1) * 4 // n % 2
				// 如果是对应的前四位，就会右移4位；
				// 与上 0000 1111得到对应的4bits
				count := (r[byteOffset] >> int8(counterOffset)) & 0x0f
				// 限制最大计数，避免新数据难以进入缓存
				if count < 15 {
					r[byteOffset] += (1 << counterOffset)
				}
			}
			func (r cmRow) get(offset int) uint8 {
				// 计算应该放在哪个byte中
				byteOffset := offset / 2
				// 计算对应的是byte的前4位还是后四位
				counterOffset := (offset & 1) * 4 // n % 2
				// 如果是对应的前四位，就会右移4位；
				// 与上 0000 1111得到对应的4bits
				count := (r[byteOffset] >> int8(counterOffset)) & 0x0f
				return count
			}	
		
		· 同时引入了一些的策略来让新数据更容易进入缓存中；
		  比如当缓存整体被访问1000次后，就认为旧的数据可能已经过时了，将所有的个数全部减半；
		  以及直接清空整个缓存的方法

		· 代码实现：
			func (r cmRow) reset() {
				for i := range r {
					// 15 9 ： (1111 1001 >> 1) == (0111 1100) & 0111 0111 == 0111 0100 = 7 4
					r[i] = (r[i] >> 1) & 0x77 // 0111 0111
				}
			}
			func (r cmRow) clear() {
				for i := range r {
					r[i] = 0
				}
			}

		· 如何快速找到一个最接近的二次幂数？
			func next2Power(x uint64) uint64 {
				// 当x是二次幂数时，这样可以保证能找到自己
				x--
				// 一个数的最高有效位肯定是1，首先或上右移1位就可以保证高2位都是1；
				// 后续同理
				x |= x >> 1
				x |= x >> 2
				x |= x >> 4
				x |= x >> 8
				x |= x >> 16
				x |= x >> 32
				// 此时可以保证所有位置都为1
				x++
				return x
			}

		· 但是，和布隆过滤器一样，这样也会出现假阳性的问题，也就是两个不同的key会在同4bits上做+1的计算；
		  为了缓解假阳性的问题，min-count sketch和bloomfilter一样，使用了k个hash函数，然后在k个值中选择了最小的count来作为最终的输出
			// 每个key都需要4个hash值
		 	 const cmDepth = 4
			// 封装一下
			type cmSketch struct {
				rows [cmDepth]cmRow
				seed [cmDepth]uint64
				mask uint64
			}
			
			//numCounter - 1 = next2Power() = 0111111(n个1）
			
			//0000,0000|0000,0000|0000,0000
			//0000,0000|0000,0000|0000,0000
			//0000,0000|0000,0000|0000,0000
			//0000,0000|0000,0000|0000,0000
			func newCmSketch(numCounters int64) *cmSketch {
				if numCounters == 0 {
					panic("cmSketch: bad numCounters")
				}
				// 得到一个最接近counter数量的二次幂数
				numCounters = next2Power(numCounters)
				// counter数量的掩码
				sketch := &cmSketch{mask: uint64(numCounters - 1)}
				// 随机一个seed种子
				source := rand.New(rand.NewSource(time.Now().UnixNano()))
				// 创建四个等大的cmRow
				for i := 0; i < cmDepth; i++ {
					sketch.seed[i] = source.Uint64()
					sketch.rows[i] = NewCmRow(numCounters)
				}
				return sketch
			}
			// 每一行都计算一个hash值，并将key打到hash对应的counter上
			func (s *cmSketch) Increment(hashed uint64) {
				for i := range s.rows {
					// 将hasd值 & counter数量-1(等效于取模)
					s.rows[i].incrRow((hashed ^ s.seed[i]) & s.mask)
				}
			}
			
			// 找到最小的计数值
			func (s *cmSketch) Estimate(hashed uint64) int64 {
				min := byte(255)
				// 每一行都get到对应的count
				for i := range s.rows {
					// 将hasd值 & counter数量-1(等效于取模)
					val := s.rows[i].get((hashed ^ s.seed[i]) & s.mask)
					// 找到其中最小的
					if val < min {
						min = val
					}
				}
				return int64(min)
			}
			
			// 让所有计数器都减半，保鲜机制
			func (s *cmSketch) Reset() {
				for _, r := range s.rows {
					r.reset()
				}
			}
			
			// 清空所有计数器
			func (s *cmSketch) Clear() {
				for _, r := range s.rows {
					r.clear()
				}
			}


三、Windoww-TinyLFU算法：	
	· TinyLFU解决了LFU统计的内存的消耗问题，和缓存保鲜的问题，但是TinyLFU不能很好的应对突发稀疏流量的情况；
	  这些系数流量的访问频率不足以让他们在LFU缓存中占据位置，很容易就被淘汰了；
	  同时LRU对稀疏流量的控制效果很好，那么就可以将LFU和LRU结合一下，于是就出现了Window-TinyLFU；
	
	· Window-TinyLFU中包括LRU和LFU两个部分，
	  前端的小LRU叫做Window LRU，它的容量只占据1%的总空间，它的作用是存放短期的突发访问数据；
	  存放主要元素的Segmented LRU(SLRU)是一种LRU的改进版，主要是把一个时间窗口内命中至少2次的记录和命中1次的单独存放，这样就可以将短期内较频繁的缓存元素区分开；
	  具体的做法是，SLRU包含两个固定尺寸的A1、A2的protection段；
	  新记录总是插入到A1中，当A1的记录再次被访问，就把它移到A2中，当A2满了需要驱逐时，会把驱逐记录插入到A1中，W-TinyLFU中，SLRU有80%的空间被分配给了A2段；

	· 所有的数据都会先进入window-lru，这里面会缓存近期访问的数据，对于突发的稀疏流量，应对的效果比较好；
	  但是window-lru的准入策略是所有数据都可以加入，所以很快就会填满，然后新进入的数据就会淘汰掉旧的数据；
	
	· WindowLRU的代码实现：
		type windowLRU struct {
			// 数据
			data map[uint64]*list.Element
			// 容量上限
			cap int
			// 用于lru淘汰策略
			list list.List
		}
		type storeItem struct {
			stage    int
			key      uint64
			conflict uint64
			value    interface{}
		}
		
		func (wl *windowLRU) add(newItem storeItem) (eItem storeItem, evicted bool) {
			// 如果没有满直接插入
			if wl.list.Len() < wl.cap {
				// 头插法
				wl.data[newItem.key] = wl.list.PushFront(&newItem)
				return storeItem{}, false
			}
			// 如果已经满了，按照LRU淘汰策略会直接从尾部淘汰
			evictItem := wl.list.Back()
			// 类型断言
			item := evictItem.Value.(*storeItem)
			// 从data map中删除数据
			delete(wl.data, item.key)
			// 直接对数据赋值
			eItem, *item = *item, newItem
			// 放入到data map中
			wl.data[item.key] = evictItem
			// 将这个Element移动到list的头
			wl.list.MoveToFront(evictItem)
			return eItem, true
		}
		// 只调节WindowLRU的顺序
		func (wl *windowLRU) get(element *list.Element) {
			// 在W-TinyLFU中不需要WindowLRU做出返回的操作
			wl.list.MoveToFront(element)
		}
		func (wl *windowLRU) String() string {
			var res string
			for e := wl.list.Front(); e != nil; e = e.Next() {
				res += fmt.Sprintf("%v", e.Value.(*storeItem).value)
			}
			return res
		}
	· Segment-lru是将缓存区分成两个部分，A1 probation占据20%空间，A2 protected占据80%的空间；
	  当数据需要进入SLRU中，都会先进入A1，当数据在A1中再次被访问，会进入到A2中；
	  如果A2中的数据满了，将会淘汰最后一个数据；

	· Segment-lru的代码实现：
		const (
			STAGE_ONE = iota + 1
			STAGE_TWO
		)
		
		type segmentedLRU struct {
			data         map[uint64]*list.Element
			a1Cap, a2Cap int
			a1, a2       *list.List
		}
		
		func (sl *segmentedLRU) Len() int {
			return sl.a1.Len() + sl.a2.Len()
		}
		func (sl *segmentedLRU) add(newItem storeItem) {
			// 先将来的都是在A1中
			newItem.stage = STAGE_ONE
		
			// 如果A1没满，且整个LFU区域也没有满
			if sl.a1.Len() < sl.a1Cap || sl.Len() < sl.a1Cap+sl.a2Cap {
				sl.data[newItem.key] = sl.a1.PushFront(&newItem)
				return
			}
		
			// 否则说明a1满了，或者整个LFU都满了
			// 那么需要从StageOne中淘汰数据
			e := sl.a1.Back()
			item := e.Value.(*storeItem)
			// 删除
			delete(sl.data, item.key)
		
			*item = newItem
			sl.data[item.key] = e
			sl.a1.MoveToFront(e)
		}
		func (sl *segmentedLRU) get(element *list.Element) {
			item := element.Value.(*storeItem)
		
			// 如果访问的数据已经在A2中了，直接将它调整位置就行了
			if item.stage == STAGE_TWO {
				sl.a2.MoveToFront(element)
				return
			}
		
			// 否则就在A1中
			// 如果可以直接放入A2，直接放入
			if sl.a2.Len() < sl.a2Cap {
				sl.a1.Remove(element)
				item.stage = STAGE_TWO
				sl.data[item.key] = sl.a2.PushFront(item)
				return
			}
		
			// 如果不能直接放入A2，就需要从A2中淘汰一些数据到A1中；
			a2Back := sl.a2.Back()
			a2Item := a2Back.Value.(*storeItem)
			// 交换
			*a2Item, *item = *item, *a2Item
			a2Item.stage = STAGE_TWO
			item.stage = STAGE_ONE
		
			// 修改data map中的数据
			sl.data[item.key] = element
			sl.data[a2Item.key] = a2Back
		
			sl.a1.MoveToFront(element)
			sl.a2.MoveToFront(a2Back)
		}
		func (sl *segmentedLRU) victim() *storeItem {
			// 如果SLRU没有满就不需要淘汰
			if sl.Len() < sl.a1Cap+sl.a2Cap {
				return nil
			}
			// 否则从A1中淘汰一个元素
			return sl.a1.Back().Value.(*storeItem)
		}
		
		func (sl *segmentedLRU) String() string {
			var res string
			for e := sl.a2.Front(); e != nil; e = e.Next() {
				res += fmt.Sprintf("%v", e.Value.(*storeItem).value)
			}
			res += fmt.Sprintf(" | ")
			for e := sl.a1.Front(); e != nil; e = e.Next() {
				res += fmt.Sprintf("%v", e.Value.(*storeItem).value)
			}
			return res
		}
		func newSLRU(data map[uint64]*list.Element, a1Cap, a2Cap int) *segmentedLRU {
			return &segmentedLRU{
				data:  data,
				a1Cap: a1Cap,
				a2Cap: a2Cap,
				a1:    list.New(),
				a2:    list.New(),
			}
		}

	· 到此，已经实现了window-lru 和 slru的逻辑，接下来就需要将两个lru结合起来；
	  因为get的逻辑不会涉及到window-lru 和 slru之间的数据流动，那么只有set操作会影响；
	  当set一个数值时，首先会进入到window-lru中，如果wlru没有满，直接add就行了；
	  如果wlru满了，就需要淘汰策略，但是淘汰后的数据不会直接丢掉，而是加入到slru的A1中；
	  如果A1也满了，就从A1中淘汰一个出去，并将这个数据插入进去；
	
	· 问题来了，如果从A1中淘汰的数据被访问的频次比较高，从wlru中淘汰的数据频次比较低怎么办？
	  可以通过cmSketch中的计数器，来统计过去访问的频率，就可以决定抛弃哪一个数据了；

	· 同时，如果大量的数据都只访问一次，那么每次从wlru中淘汰的数据，会被频繁的淘汰出来，那么每一次都要和slru中的数据做对比；
	  也就是说windowLRU如果已经满了就会淘汰一个数据出来，出来的数据会尝试加入到SLRU中，因为wlru很容易满，所以会频繁的触发对比；
	  为了优化这种情况，会在加入SLRU之前结果一层BloomFilter的拦截，如果存在于SLRU中就认为可能会被访问多次，才会触发对比机制，并添加到BloomFilter中；

	· 但是到这里可以发现LFU是把频次高的放在前面，但是tinylfu的slfu还是个lru，并没有根据频次排序；
	  那么这样频次高的数据仍然很可能会在SLRU中被淘汰，这样不就违背了LFU的原则了吗；
	  实际上，一个高频数据被淘汰出去，在和WLRU中对比时会被保留下来；
	  如果仍然被淘汰了，如果后面没有访问，那么他的淘汰是应该的，
	  如果后面又被访问到，由于他的计数信息还存在(BloomFilter 和 cmSketch)，所以极大概率会被重新添加到缓存中；

	· 所以对于所有类型的数据，缓存只有以下几种情况：
		1. 针对只访问一次的数据，在WLRU中很快就被淘汰了，不会占用缓存空间；
		2. 针对突发的稀疏流量，就是可能会在短时间内频繁访问的数据，WLRU可以很好的适应这种访问模式；
		3. 真正的热点数据，很快就会从WLRU加入到Proteeced A2区域中，并且会通过reset保险机制维持缓存的活性；


四、Cache


五、问题日志：
	1. list没有使用 *list.List 使用的是list.List导致 sl.a1.Back().Value.(*storeItem)报错，value为nil不是storeItem；
	   原因是直接使用list.List会自带一个value为nil的element，导致获取时候报错